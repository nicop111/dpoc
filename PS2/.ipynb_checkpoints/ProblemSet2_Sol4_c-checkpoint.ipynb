{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming and Optimal Control\n",
    "## Problem Set 2, Problem 4.c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python script that solves Problem 4.c of Problem Set 2 by applying the value iteration. Problem is taken from the book \"Dynamic Programming and Optimal Control\", Vol. 1, by D. Bertsekas. (Page 446, Problem 7.3c)** \n",
    "\n",
    "**We use [NumPy](https://numpy.org/) and [matplotlib](https://matplotlib.org/) packages. You can install these packages using `pip install` or `conda install` command depending on your package manager. You can also find the installation guide in the package websites or documentations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITER = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup problem data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage rewards g. g is a 2x2 matrix where the first index correponds to the\n",
    "# state i=0,1 and the second index correponds to the admissible control\n",
    "# input u=0 (=advertising/do research),\n",
    "# or 1 (=don't advertise/don't do research).\n",
    "g = np.array([[4, 6], [-5, -3]])\n",
    "\n",
    "# Transition probabilities. P is a 2x2x2 matrix, where the first index\n",
    "# corresponds to the origin state, the second index corresponds to the\n",
    "# destination state and the third input corresponds to the applied control\n",
    "# input where (0,1) maps to (advertise/do research, don't advertise/don't\n",
    "# do research).  For example, the probability of transition from node 0 to\n",
    "# node 1 given that we do not advertice is P[0,1,1].\n",
    "P = np.zeros([2, 2, 2])\n",
    "\n",
    "# Advertise/do research (u=0):\n",
    "P[:, :, 0] = np.array([[0.8, 0.2], [0.7, 0.3]])\n",
    "\n",
    "# Don't advertise/don't do research (u=1):\n",
    "P[:, :, 1] = np.array([[0.5, 0.5], [0.4, 0.6]])\n",
    "\n",
    "# Discount factor.\n",
    "alpha = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables that we update during value iteration.\n",
    "# Cost (here it really is the reward):\n",
    "costJ = np.array([0, 0], dtype=np.float32)\n",
    "costJnew = np.array([0, 0], dtype=np.float32)\n",
    "\n",
    "# Variable to save results\n",
    "res_J = np.ndarray([NUM_ITER, 2])\n",
    "\n",
    "# Policy\n",
    "policy = np.array([0, 0])\n",
    "\n",
    "# Loop over value iterations k\n",
    "for k in range(NUM_ITER):\n",
    "    for i in range(2):  # loop over two states\n",
    "        # One value iteration step for each state\n",
    "        # We use max because we work with rewards instead of costs\n",
    "        costJnew[i] = np.max(g[i, :] + alpha * P[i, :, :].T @ costJ)\n",
    "        policy[i] = np.argmax(g[i, :] + alpha * P[i, :, :].T @ costJ)\n",
    "\n",
    "    # Save results for plotting later.\n",
    "    res_J[k, :] = costJnew\n",
    "\n",
    "    # Update the cost\n",
    "    costJ = costJnew\n",
    "\n",
    "    # Display results\n",
    "    print(\n",
    "        \"k=\",\n",
    "        format(k),\n",
    "        \"   J(0)=\",\n",
    "        format(costJnew[0], \".4f\"),\n",
    "        \"   J(1)=\",\n",
    "        format(costJnew[1], \".4f\"),\n",
    "        \"   mu(0)=\",\n",
    "        format(policy[0]),\n",
    "        \"   mu(1)=\",\n",
    "        format(\n",
    "            policy[1],\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the optained costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Result:\\n\",\n",
    "    \"  J*(0) = \",\n",
    "    format(costJ[0], \".4f\"),\n",
    "    \"\\n\",\n",
    "    \"  J*(1) = \",\n",
    "    format(costJ[1], \".4f\"),\n",
    "    \"\\n\",\n",
    "    \"  mu*(0) = \",\n",
    "    format(policy[0]),\n",
    "    \"\\n\",\n",
    "    \"  mu*(1) = \",\n",
    "    format(policy[1]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot costs over iterations.\n",
    "fig, (ax1, ax2) = plt.subplots(2)\n",
    "ax1.plot(np.array(range(NUM_ITER)) + 1, res_J[:, 0], \"b\")\n",
    "ax1.plot(np.array(range(NUM_ITER)) + 1, np.tile(costJ[0], NUM_ITER), \"k\")\n",
    "ax1.legend([\"J_k(0)\", \"J*(0)\"], loc=\"upper left\", bbox_to_anchor=(1.04, 1))\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(np.array(range(NUM_ITER)) + 1, res_J[:, 1], \"b\")\n",
    "ax2.plot(np.array(range(NUM_ITER)) + 1, np.tile(costJ[1], NUM_ITER), \"k\")\n",
    "ax2.legend([\"J_k(1)\", \"J*(1)\"], loc=\"upper left\", bbox_to_anchor=(1.04, 1))\n",
    "ax2.grid(True)\n",
    "ax2.set_xlabel(\"Iteration\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "cff752820ff3f6d3fa9162deadbdb76123777b3fbf2c04670d7e4bc2026a6053"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
